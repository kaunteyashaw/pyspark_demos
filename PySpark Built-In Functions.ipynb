{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d5faa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Part3\").getOrCreate()\n",
    "sc= spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7322a048",
   "metadata": {},
   "source": [
    "    PySpark supports a way to check multiple conditions in sequence and returns a value when the first condition met by using SQL like case when and when().otherwise() expressions, these works similar to “Switch\" and \"if then else\" statements.\n",
    "    \n",
    "* Using “When Otherwise” on DataFrame.\n",
    "* PySpark SQL “Case When” on DataFrame.\n",
    "* Using Multiple Conditions With & (And) | (OR) operators\n",
    "\n",
    "    PySpark When Otherwise – when() is a SQL function that returns a Column type and otherwise() is a function of Column, if otherwise() is not used, it returns a None/NULL value.\n",
    "\n",
    "    PySpark SQL Case When – This is similar to SQL expression, Usage: CASE WHEN cond1 THEN result WHEN cond2 THEN result... ELSE result END."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "766ae6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|   name|gender|salary|\n",
      "+-------+------+------+\n",
      "|  James|     M| 60000|\n",
      "|Michael|     M| 70000|\n",
      "| Robert|  null|400000|\n",
      "|  Maria|     F|500000|\n",
      "|    Jen|      |  null|\n",
      "+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"M\",60000),(\"Michael\",\"M\",70000),\n",
    "        (\"Robert\",None,400000),(\"Maria\",\"F\",500000),\n",
    "        (\"Jen\",\"\",None)]\n",
    "\n",
    "columns = [\"name\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2a8dfc",
   "metadata": {},
   "source": [
    "    1. Using when() otherwise() on PySpark DataFrame.\n",
    "PySpark when() is SQL function, in order to use this first you should import and this returns a Column type, otherwise() is a function of Column, when otherwise() not used and none of the conditions met it assigns None (Null) value. Usage would be like when(condition).otherwise(default).\n",
    "\n",
    "when() function take 2 parameters, first param takes a condition and second takes a literal value or Column, if condition evaluates to true then it returns a value from second param.\n",
    "\n",
    "The below code snippet replaces the value of gender with a new derived value, when conditions not matched, we are assigning “Unknown” as value, for null assigning empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2318337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n",
      "|   name|gender|salary|new_gender|\n",
      "+-------+------+------+----------+\n",
      "|  James|     M| 60000|      Male|\n",
      "|Michael|     M| 70000|      Male|\n",
      "| Robert|  null|400000|          |\n",
      "|  Maria|     F|500000|    Female|\n",
      "|    Jen|      |  null|          |\n",
      "+-------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import when, col\n",
    "df2 = df.withColumn(\"new_gender\", when(df.gender == \"M\",\"Male\")\n",
    "                                 .when(df.gender == \"F\",\"Female\")\n",
    "                                 .when(df.gender.isNull() ,\"\")\n",
    "                                 .otherwise(df.gender))\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17cef01",
   "metadata": {},
   "source": [
    "Using with select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eeb9d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n",
      "|   name|gender|salary|new_gender|\n",
      "+-------+------+------+----------+\n",
      "|  James|     M| 60000|      Male|\n",
      "|Michael|     M| 70000|      Male|\n",
      "| Robert|  null|400000|          |\n",
      "|  Maria|     F|500000|    Female|\n",
      "|    Jen|      |  null|          |\n",
      "+-------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df2=df.select(col(\"*\"),when(df.gender == \"M\",\"Male\").when(df.gender == \"F\",\"Female\").when(df.gender.isNull() ,\"\").otherwise(df.gender).alias(\"new_gender\"))\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3849ab85",
   "metadata": {},
   "source": [
    "2. PySpark SQL Case When on DataFrame.<br>\n",
    "    If you have a SQL background you might have familiar with Case When statement that is used to execute a sequence of conditions and returns a value when the first condition met, similar to SWITH and IF THEN ELSE statements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52549325",
   "metadata": {},
   "source": [
    "    CASE is the start of the expression\n",
    "    Clause WHEN takes a condition, if condition true it returns a value from THEN\n",
    "    If the condition is false it goes to the next condition and so on.\n",
    "    If none of the condition matches, it returns a value from the ELSE clause.\n",
    "    END is to end the expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65221c3f",
   "metadata": {},
   "source": [
    "    2.1 Using Case When Else on DataFrame using withColumn() & select()\n",
    "    Below example uses PySpark SQL expr() Function to express SQL like expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79a9ba02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n",
      "|name   |gender|salary|new_gender|\n",
      "+-------+------+------+----------+\n",
      "|James  |M     |60000 |Male      |\n",
      "|Michael|M     |70000 |Male      |\n",
      "|Robert |null  |400000|          |\n",
      "|Maria  |F     |500000|Female    |\n",
      "|Jen    |      |null  |          |\n",
      "+-------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import expr, col\n",
    "\n",
    "#Using Case When on withColumn()\n",
    "df3 = df.withColumn(\"new_gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' \" + \n",
    "               \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n",
    "               \"ELSE gender END\"))\n",
    "df3.show(truncate=False)\n",
    "\"\"\"+-------+------+------+----------+\n",
    "|name   |gender|salary|new_gender|\n",
    "+-------+------+------+----------+\n",
    "|James  |M     |60000 |Male      |\n",
    "|Michael|M     |70000 |Male      |\n",
    "|Robert |null  |400000|          |\n",
    "|Maria  |F     |500000|Female    |\n",
    "|Jen    |      |null  |          |\n",
    "+-------+------+------+----------+\n",
    "\"\"\"\n",
    "#Using Case When on select()\n",
    "df4 = df.select(col(\"*\"), expr(\"CASE WHEN gender = 'M' THEN 'Male' \" +\n",
    "           \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n",
    "           \"ELSE gender END\").alias(\"new_gender\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1172137",
   "metadata": {},
   "source": [
    "    2.2 Using Case When on SQL Expression\n",
    "You can also use Case When with SQL statement after creating a temporary view. This returns a similar output as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e144a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|   name|new_gender|\n",
      "+-------+----------+\n",
      "|  James|      Male|\n",
      "|Michael|      Male|\n",
      "| Robert|         -|\n",
      "|  Maria|    Female|\n",
      "|    Jen|        __|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.createOrReplaceTempView(\"EMP\")\n",
    "spark.sql(\"select name, CASE WHEN gender = 'M' THEN 'Male' \" + \n",
    "               \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN '-' WHEN gender='' THEN '__'\" +\n",
    "              \"ELSE gender END as new_gender from EMP\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79491b60",
   "metadata": {},
   "source": [
    "# PySpark SQL expr() (Expression ) Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d44ca4",
   "metadata": {},
   "source": [
    "PySpark ```expr()``` is a SQL function to execute SQL-like expressions and to use an existing DataFrame column value as an expression argument to Pyspark built-in functions. Most of the commonly used SQL functions are either part of the PySpark Column class or built-in pyspark.sql.functions API, besides these PySpark also supports many other SQL functions, so in order to use these, you have to use expr() function.\n",
    "\n",
    "Below are 2 use cases of PySpark expr() funcion.\n",
    "\n",
    "* First, allowing to use of SQL-like functions that are not present in PySpark Column type & pyspark.sql.functions API. for example CASE WHEN, regr_count().\n",
    "* Second, it extends the PySpark SQL Functions by allowing to use DataFrame columns in functions for expression. for example, if you wanted to add a month value from a column to a Date column. I will explain this in the example below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec63a85e",
   "metadata": {},
   "source": [
    "#### Concatenate Columns using || (similar to SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24ff867c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+\n",
      "| col1| col2|       Name|\n",
      "+-----+-----+-----------+\n",
      "|James| Bond| James,Bond|\n",
      "|Scott|Varsa|Scott,Varsa|\n",
      "+-----+-----+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n+-----+-----+-----------+\\n| col1| col2|       Name|\\n+-----+-----+-----------+\\n|James| Bond| James,Bond|\\n|Scott|Varsa|Scott,Varsa|\\n+-----+-----+-----------+\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Concatenate columns using || (sql like)\n",
    "data=[(\"James\",\"Bond\"),(\"Scott\",\"Varsa\")] \n",
    "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\") \n",
    "df.withColumn(\"Name\",expr(\" col1 ||','|| col2\")).show()\n",
    "\"\"\"\n",
    "+-----+-----+-----------+\n",
    "| col1| col2|       Name|\n",
    "+-----+-----+-----------+\n",
    "|James| Bond| James,Bond|\n",
    "|Scott|Varsa|Scott,Varsa|\n",
    "+-----+-----+-----------+\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f41ab6",
   "metadata": {},
   "source": [
    "#### Using SQL CASE WHEN with expr()\n",
    "\n",
    "\n",
    "PySpark doesn’t have SQL Like CASE WHEN so in order to use this on PySpark DataFrame withColumn() or select(), you should use expr() function with expression as shown below.\n",
    "\n",
    "Here, I have used CASE WHEN expression on withColumn() by using expr(), this example updates an existing column gender with the derived values, M for male, F for Female, and unknown for others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "761b2c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|   name| gender|\n",
      "+-------+-------+\n",
      "|  James|   Male|\n",
      "|Michael| Female|\n",
      "|    Jen|unknown|\n",
      "+-------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n+-------+-------+\\n|   name| gender|\\n+-------+-------+\\n|  James|   Male|\\n|Michael| Female|\\n|    Jen|unknown|\\n+-------+-------+\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import expr\n",
    "data = [(\"James\",\"M\"),(\"Michael\",\"F\"),(\"Jen\",\"\")]\n",
    "columns = [\"name\",\"gender\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "#Using CASE WHEN similar to SQL.\n",
    "from pyspark.sql.functions import expr\n",
    "df2=df.withColumn(\"gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' \" +\n",
    "           \"WHEN gender = 'F' THEN 'Female' ELSE 'unknown' END\"))\n",
    "df2.show()\n",
    "\n",
    "\"\"\"\n",
    "+-------+-------+\n",
    "|   name| gender|\n",
    "+-------+-------+\n",
    "|  James|   Male|\n",
    "|Michael| Female|\n",
    "|    Jen|unknown|\n",
    "+-------+-------+\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0f09e8",
   "metadata": {},
   "source": [
    "#### Using an Existing Column Value for Expression\n",
    "\n",
    "Most of the PySpark function takes constant literal values but sometimes we need to use a value from an existing column instead of a constant and this is not possible without expr() expression. The below example adds a number of months from an existing column instead of a Python constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a87c3087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+\n",
      "|      date|increment|  inc_date|\n",
      "+----------+---------+----------+\n",
      "|2019-01-23|        1|2019-02-23|\n",
      "|2019-06-24|        2|2019-08-24|\n",
      "|2019-09-20|        3|2019-12-20|\n",
      "+----------+---------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n+----------+---------+----------+\\n|      date|increment|  inc_date|\\n+----------+---------+----------+\\n|2019-01-23|        1|2019-02-23|\\n|2019-06-24|        2|2019-08-24|\\n|2019-09-20|        3|2019-12-20|\\n+----------+---------+----------+\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import expr\n",
    "data=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)] \n",
    "df=spark.createDataFrame(data).toDF(\"date\",\"increment\") \n",
    "\n",
    "#Add Month value from another column\n",
    "df.select(df.date,df.increment,\n",
    "     expr(\"add_months(date,increment)\")\n",
    "  .alias(\"inc_date\")).show()\n",
    "\"\"\"\n",
    "+----------+---------+----------+\n",
    "|      date|increment|  inc_date|\n",
    "+----------+---------+----------+\n",
    "|2019-01-23|        1|2019-02-23|\n",
    "|2019-06-24|        2|2019-08-24|\n",
    "|2019-09-20|        3|2019-12-20|\n",
    "+----------+---------+----------+\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93d0d50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "| 100|   2|\n",
      "+----+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n+----+----+\\n|col1|col2|\\n+----+----+\\n| 500| 500|\\n+----+----+\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Use expr()  to filter the rows\n",
    "from pyspark.sql.functions import expr\n",
    "data=[(100,2),(200,3000),(500,500)] \n",
    "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\") \n",
    "df.filter(expr(\"col1 == 100\")).show()\n",
    "\"\"\"\n",
    "+----+----+\n",
    "|col1|col2|\n",
    "+----+----+\n",
    "| 500| 500|\n",
    "+----+----+\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e050317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- dob_year: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+--------------------+--------+------+------+\n",
      "|name                |dob_year|gender|salary|\n",
      "+--------------------+--------+------+------+\n",
      "|James, A, Smith     |2018    |M     |3000  |\n",
      "|Michael, Rose, Jones|2010    |M     |4000  |\n",
      "|Robert,K,Williams   |2010    |M     |4000  |\n",
      "|Maria,Anne,Jones    |2005    |F     |4000  |\n",
      "|Jen,Mary,Brown      |2010    |      |-1    |\n",
      "+--------------------+--------+------+------+\n",
      "\n",
      "root\n",
      " |-- NameArray: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+--------------------+\n",
      "|           NameArray|\n",
      "+--------------------+\n",
      "| [James,  A,  Smith]|\n",
      "|[Michael,  Rose, ...|\n",
      "|[Robert, K, Willi...|\n",
      "|[Maria, Anne, Jones]|\n",
      "|  [Jen, Mary, Brown]|\n",
      "+--------------------+\n",
      "\n",
      "+--------------------+\n",
      "|           NameArray|\n",
      "+--------------------+\n",
      "| [James,  A,  Smith]|\n",
      "|[Michael,  Rose, ...|\n",
      "|[Robert, K, Willi...|\n",
      "|[Maria, Anne, Jones]|\n",
      "|  [Jen, Mary, Brown]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "         .appName('SparkByExamples.com') \\\n",
    "         .getOrCreate()\n",
    "\n",
    "data = [(\"James, A, Smith\",\"2018\",\"M\",3000),\n",
    "            (\"Michael, Rose, Jones\",\"2010\",\"M\",4000),\n",
    "            (\"Robert,K,Williams\",\"2010\",\"M\",4000),\n",
    "            (\"Maria,Anne,Jones\",\"2005\",\"F\",4000),\n",
    "            (\"Jen,Mary,Brown\",\"2010\",\"\",-1)\n",
    "            ]\n",
    "\n",
    "columns=[\"name\",\"dob_year\",\"gender\",\"salary\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import split, col\n",
    "df2 = df.select(split(col(\"name\"),\",\").alias(\"NameArray\")) \\\n",
    "    .drop(\"name\")\n",
    "df2.printSchema()\n",
    "df2.show()\n",
    "\n",
    "df.createOrReplaceTempView(\"PERSON\")\n",
    "df3= spark.sql(\"select SPLIT(name,',') as NameArray from PERSON\") \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a60c0f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           NameArray|\n",
      "+--------------------+\n",
      "| [James,  A,  Smith]|\n",
      "|[Michael,  Rose, ...|\n",
      "|[Robert, K, Willi...|\n",
      "|[Maria, Anne, Jones]|\n",
      "|  [Jen, Mary, Brown]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"PERSON\")\n",
    "df3= spark.sql(\"select SPLIT(name,',') as NameArray from PERSON\")\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5897237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           NameArray|\n",
      "+--------------------+\n",
      "| [James,  A,  Smith]|\n",
      "|[Michael,  Rose, ...|\n",
      "|[Robert, K, Willi...|\n",
      "|[Maria, Anne, Jones]|\n",
      "|  [Jen, Mary, Brown]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select SPLIT(name,',') as NameArray from PERSON\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482a2a47",
   "metadata": {},
   "source": [
    "#### PySpark Replace Column Values in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e42e4fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+\n",
      "| id|           address|state|\n",
      "+---+------------------+-----+\n",
      "|  1|  14851 Jeffrey Rd|   DE|\n",
      "|  2|43421 Margarita St|   NY|\n",
      "|  3|  13111 Siemon Ave|   CA|\n",
      "+---+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "address = [(1,\"14851 Jeffrey Rd\",\"DE\"),\n",
    "    (2,\"43421 Margarita St\",\"NY\"),\n",
    "    (3,\"13111 Siemon Ave\",\"CA\")]\n",
    "df =spark.createDataFrame(address,[\"id\",\"address\",\"state\"])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2acdd08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52579557",
   "metadata": {},
   "source": [
    "    1. PySpark Replace String Column Values\n",
    "By using PySpark SQL function regexp_replace() you can replace a column value with a string for another string/substring. regexp_replace() uses Java regex for matching, if the regex does not match it returns an empty string, the below example replace the street name Rd value with Road string on address column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f13d366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+\n",
      "|id |address           |state|\n",
      "+---+------------------+-----+\n",
      "|1  |14851 Jeffrey Road|DE   |\n",
      "|2  |43421 Margarita St|NY   |\n",
      "|3  |13111 Siemon Ave  |CA   |\n",
      "+---+------------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n+---+------------------+-----+\\n|id |address           |state|\\n+---+------------------+-----+\\n|1  |14851 Jeffrey Road|DE   |\\n|2  |43421 Margarita St|NY   |\\n|3  |13111 Siemon Ave  |CA   |\\n+---+------------------+-----+\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Replace part of string with another string\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "df.withColumn('address', regexp_replace('address', 'Rd', 'Road')) \\\n",
    "  .show(truncate=False)\n",
    "\"\"\"\n",
    "+---+------------------+-----+\n",
    "|id |address           |state|\n",
    "+---+------------------+-----+\n",
    "|1  |14851 Jeffrey Road|DE   |\n",
    "|2  |43421 Margarita St|NY   |\n",
    "|3  |13111 Siemon Ave  |CA   |\n",
    "+---+------------------+-----+\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "932b3e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------+-----+\n",
      "|id |address               |state|\n",
      "+---+----------------------+-----+\n",
      "|1  |14851 Jeffrey Road    |DE   |\n",
      "|2  |43421 Margarita Street|NY   |\n",
      "|3  |13111 Siemon Avenue   |CA   |\n",
      "+---+----------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Replace string column value conditionally\n",
    "from pyspark.sql.functions import when\n",
    "df.withColumn('address', \n",
    "    when(df.address.endswith('Rd'),regexp_replace(df.address,'Rd','Road')) \\\n",
    "   .when(df.address.endswith('St'),regexp_replace(df.address,'St','Street')) \\\n",
    "   .when(df.address.endswith('Ave'),regexp_replace(df.address,'Ave','Avenue')) \\\n",
    "   .otherwise(df.address)) \\\n",
    "   .show(truncate=False)\n",
    "\n",
    "#+---+----------------------+-----+\n",
    "#|id |address               |state|\n",
    "#+---+----------------------+-----+\n",
    "#|1  |14851 Jeffrey Road    |DE   |\n",
    "#|2  |43421 Margarita Street|NY   |\n",
    "#|3  |13111 Siemon Avenue   |CA   |\n",
    "#+---+----------------------+-----+ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bbc820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c31477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104168e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea664dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
