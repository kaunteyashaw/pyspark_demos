{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39d52431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataDF = [(('James','','Smith'),'1991-04-01','M',3000),\n",
    "  (('Michael','Rose',''),'2000-05-19','M',4000),\n",
    "  (('Robert','','Williams'),'1978-09-05','M',4000),\n",
    "  (('Maria','Anne','Jones'),'1967-12-01','F',4000),\n",
    "  (('Jen','Mary','Brown'),'1980-02-17','F',-1)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9322115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('dob', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', IntegerType(), True)\n",
    "         ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2916e6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "df = spark.createDataFrame(data = dataDF, schema = schema)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30b1bec",
   "metadata": {},
   "source": [
    "1. PySpark withColumnRenamed – To rename DataFrame column name\n",
    "\n",
    "\n",
    "* withColumnRenamed(existingName, newNam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "612e63db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.withColumnRenamed(\"dob\",\"DateOfBirth\").printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb5d047",
   "metadata": {},
   "source": [
    "2. PySpark withColumnRenamed – To rename multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bd408c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary_amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df2 = df.withColumnRenamed(\"dob\",\"DateOfBirth\") \\\n",
    "        .withColumnRenamed(\"salary\",\"salary_amount\")\n",
    "\n",
    "df2.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45f7c04",
   "metadata": {},
   "source": [
    "3. Using PySpark StructType – To rename a nested column in Dataframe\n",
    "\n",
    "* Changing a column name on nested data is not straight forward and we can do this by creating a new schema with new DataFrame columns using StructType and use it using cast function as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3183e06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- fname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "schema2 = StructType([\n",
    "    StructField(\"fname\",StringType()),\n",
    "    StructField(\"middlename\",StringType()),\n",
    "    StructField(\"lname\",StringType())])\n",
    "#casting to the new schema\n",
    "df.select(col(\"name\").cast(schema2), col(\"dob\"), col(\"gender\"),col(\"salary\")).printSchema() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c3e55c",
   "metadata": {},
   "source": [
    "4. Using Select – To rename nested elements. -- using alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a074a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FIRSTNAME: string (nullable = true)\n",
      " |-- MIDDLENAME: string (nullable = true)\n",
      " |-- LASTNAME: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import *\n",
    "df.select(col(\"name.firstname\").alias(\"FIRSTNAME\"), \\\n",
    "  col(\"name.middlename\").alias(\"MIDDLENAME\"), \\\n",
    "  col(\"name.lastname\").alias(\"LASTNAME\"), \\\n",
    "  col(\"dob\").alias(\"DateOfBirth\"),col(\"gender\"),col(\"salary\")) \\\n",
    "  .printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595b1252",
   "metadata": {},
   "source": [
    "5. Using PySpark DataFrame withColumn – To rename nested columns\n",
    "\n",
    "* When you have nested columns on PySpark DatFrame and if you want to rename it, use withColumn on a data frame object to create a new column from an existing and we will need to drop the existing column. Below example creates a “fname” column from “name.firstname” and drops the “name” column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd7659ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- fnamexx: string (nullable = true)\n",
      " |-- mnamexx: string (nullable = true)\n",
      " |-- lnamexx: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df4 = df.withColumn(\"fnamexx\",col(\"name.firstname\")) \\\n",
    "      .withColumn(\"mnamexx\",col(\"name.middlename\")) \\\n",
    "      .withColumn(\"lnamexx\",col(\"name.lastname\")) \\\n",
    "      .drop(\"name\")\n",
    "df4.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4317d606",
   "metadata": {},
   "source": [
    "6. Using col() function – To Dynamically rename all or multiple columns\n",
    "* Another way to change all column names on Dataframe is to use col() function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e305781",
   "metadata": {},
   "source": [
    "7. Using toDF() – To change all columns in a PySpark DataFrame\n",
    "* When we have data in a flat structure (without nested) , use toDF() with a new schema to change all column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2ea110f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- newCol1: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- newCol2: string (nullable = true)\n",
      " |-- newCol3: string (nullable = true)\n",
      " |-- newCol4: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "newColumns = [\"newCol1\",\"newCol2\",\"newCol3\",\"newCol4\"]\n",
    "df.toDF(*newColumns).printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aabc28",
   "metadata": {},
   "source": [
    "# filter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b4b696",
   "metadata": {},
   "source": [
    "1. PySpark DataFrame filter() Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a9202eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- languages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
      "|{Anna, Rose, }        |[Spark, Java, C++]|NY   |F     |\n",
      "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
      "|{Maria, Anne, Jones}  |[CSharp, VB]      |NY   |M     |\n",
      "|{Jen, Mary, Brown}    |[CSharp, VB]      |NY   |M     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType,StructField \n",
    "from pyspark.sql.types import StringType, IntegerType, ArrayType\n",
    "data = [\n",
    "    ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\"),\n",
    "    ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n",
    "    ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "    ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")\n",
    " ]\n",
    "        \n",
    "schema = StructType([\n",
    "     StructField('name', StructType([\n",
    "        StructField('firstname', StringType(), True),\n",
    "        StructField('middlename', StringType(), True),\n",
    "         StructField('lastname', StringType(), True)\n",
    "     ])),\n",
    "     StructField('languages', ArrayType(StringType()), True),\n",
    "     StructField('state', StringType(), True),\n",
    "     StructField('gender', StringType(), True)\n",
    " ])\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee38fb1",
   "metadata": {},
   "source": [
    "2. DataFrame filter() with Column Conditio\n",
    "* Use Column with the condition to filter the rows from DataFrame, using this you can express complex condition by referring column names using dfObject.colname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bbf88ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
      "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n",
      "+--------------------+------------------+-----+------+\n",
      "|name                |languages         |state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|{Anna, Rose, }      |[Spark, Java, C++]|NY   |F     |\n",
      "|{Maria, Anne, Jones}|[CSharp, VB]      |NY   |M     |\n",
      "|{Jen, Mary, Brown}  |[CSharp, VB]      |NY   |M     |\n",
      "+--------------------+------------------+-----+------+\n",
      "\n",
      "+--------------------+------------------+-----+------+\n",
      "|name                |languages         |state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|{Anna, Rose, }      |[Spark, Java, C++]|NY   |F     |\n",
      "|{Maria, Anne, Jones}|[CSharp, VB]      |NY   |M     |\n",
      "|{Jen, Mary, Brown}  |[CSharp, VB]      |NY   |M     |\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Using equals condition\n",
    "df.filter(df.state == \"OH\").show(truncate=False)\n",
    "\n",
    "# +----------------------+------------------+-----+------+\n",
    "# |name                  |languages         |state|gender|\n",
    "# +----------------------+------------------+-----+------+\n",
    "# |[James, , Smith]      |[Java, Scala, C++]|OH   |M     |\n",
    "# |[Julia, , Williams]   |[CSharp, VB]      |OH   |F     |\n",
    "# |[Mike, Mary, Williams]|[Python, VB]      |OH   |M     |\n",
    "# +----------------------+------------------+-----+------+\n",
    "\n",
    "# not equals condition\n",
    "df.filter(df.state != \"OH\") \\\n",
    "    .show(truncate=False) \n",
    "df.filter(~(df.state == \"OH\")) \\\n",
    "    .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0738db7e",
   "metadata": {},
   "source": [
    "Same example can also written as below. In order to use this first you need to import from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a88a2aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
      "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Using SQL col() function\n",
    "from pyspark.sql.functions import col\n",
    "df.filter(col(\"state\") == \"OH\") \\\n",
    "    .show(truncate=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdbb41c",
   "metadata": {},
   "source": [
    "3. DataFrame filter() with SQL Expression\n",
    "* If you are coming from SQL background, you can use that knowledge in PySpark to filter DataFrame rows with SQL expressions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "616667e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|\n",
      "|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n",
      "|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n",
      "|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n",
      "+-------------------+------------------+-----+------+\n",
      "|               name|         languages|state|gender|\n",
      "+-------------------+------------------+-----+------+\n",
      "|     {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n",
      "|{Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n",
      "+-------------------+------------------+-----+------+\n",
      "\n",
      "+-------------------+------------------+-----+------+\n",
      "|               name|         languages|state|gender|\n",
      "+-------------------+------------------+-----+------+\n",
      "|     {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n",
      "|{Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n",
      "+-------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Using SQL Expression\n",
    "df.filter(\"gender == 'M'\").show()\n",
    "#For not equal\n",
    "df.filter(\"gender != 'M'\").show()\n",
    "df.filter(\"gender <> 'M'\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af29b8",
   "metadata": {},
   "source": [
    "4. PySpark Filter with Multiple Conditions\n",
    "* In PySpark, to filter() rows on DataFrame based on multiple conditions, you case use either Column with a condition or SQL expression. Below is just a simple example using AND (&) condition, you can extend this with OR(|), and NOT(!) conditional expressions as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08cc855e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter multiple condition\n",
    "df.filter( (df.state  == \"OH\") & (df.gender  == \"M\") ) \\\n",
    "    .show(truncate=False)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c45100",
   "metadata": {},
   "source": [
    "5. Filter Based on List Values\n",
    "* If you have a list of elements and you wanted to filter that is not in the list or in the list, use isin() function of Column class and it doesn’t have isnotin() function but you do the same using not operator (~)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be2334c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|\n",
      "| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n",
      "|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n",
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n",
      "|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n",
      "|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n",
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n",
      "|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n",
      "|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Filter IS IN List values\n",
    "li=[\"OH\",\"CA\",\"DE\"]\n",
    "df.filter(df.state.isin(li)).show()\n",
    "# +--------------------+------------------+-----+------+\n",
    "# |                name|         languages|state|gender|\n",
    "# +--------------------+------------------+-----+------+\n",
    "# |    [James, , Smith]|[Java, Scala, C++]|   OH|     M|\n",
    "# | [Julia, , Williams]|      [CSharp, VB]|   OH|     F|\n",
    "# |[Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n",
    "# +--------------------+------------------+-----+------+\n",
    "\n",
    "# Filter NOT IS IN List values\n",
    "#These show all records with NY (NY is not part of the list)\n",
    "df.filter(~df.state.isin(li)).show()\n",
    "df.filter(df.state.isin(li)==False).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b389b0c7",
   "metadata": {},
   "source": [
    "6. Filter Based on Starts With, Ends With, Contains\n",
    "* You can also filter DataFrame rows by using startswith(), endswith() and contains() methods of Column class. For more examples on Column class, refer to PySpark Column Functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d820449d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n",
      "|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n",
      "|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n",
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|\n",
      "| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n",
      "|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n",
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|\n",
      "| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n",
      "|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Using startswith\n",
    "df.filter(df.state.startswith(\"N\")).show()\n",
    "# +--------------------+------------------+-----+------+\n",
    "# |                name|         languages|state|gender|\n",
    "# +--------------------+------------------+-----+------+\n",
    "# |      [Anna, Rose, ]|[Spark, Java, C++]|   NY|     F|\n",
    "# |[Maria, Anne, Jones]|      [CSharp, VB]|   NY|     M|\n",
    "# |  [Jen, Mary, Brown]|      [CSharp, VB]|   NY|     M|\n",
    "# +--------------------+------------------+-----+------+\n",
    "\n",
    "#using endswith\n",
    "df.filter(df.state.endswith(\"H\")).show()\n",
    "\n",
    "#contains\n",
    "df.filter(df.state.contains(\"H\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1651bc",
   "metadata": {},
   "source": [
    "7. PySpark Filter like and rlike\n",
    "* If you have SQL background you must be familiar with ```like``` and ```rlike (regex like)```, PySpark also provides similar methods in Column class to filter similar values using wildcard characters. You can use rlike() to filter by checking values case insensitive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5d64403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|      name|\n",
      "+---+----------+\n",
      "|  5|Rames rose|\n",
      "+---+----------+\n",
      "\n",
      "+---+------------+\n",
      "| id|        name|\n",
      "+---+------------+\n",
      "|  2|Michael Rose|\n",
      "|  4|  Rames Rose|\n",
      "|  5|  Rames rose|\n",
      "+---+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'+---+------------+\\n| id|        name|\\n+---+------------+\\n|  2|Michael Rose|\\n|  4|  Rames Rose|\\n|  5|  Rames rose|\\n+---+------------+'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data2 = [(2,\"Michael Rose\"),(3,\"Robert Williams\"),\n",
    "     (4,\"Rames Rose\"),(5,\"Rames rose\")\n",
    "  ]\n",
    "df2 = spark.createDataFrame(data = data2, schema = [\"id\",\"name\"])\n",
    "\n",
    "# like - SQL LIKE pattern\n",
    "df2.filter(df2.name.like(\"%rose%\")).show()\n",
    "\"\"\"+---+----------+\n",
    "| id|      name|\n",
    "+---+----------+\n",
    "|  5|Rames rose|\n",
    "+---+----------+\"\"\"\n",
    "\n",
    "# rlike - SQL RLIKE pattern (LIKE with Regex)\n",
    "#This check case insensitive\n",
    "df2.filter(df2.name.rlike(\"(?i)^*rose$\")).show()\n",
    "\"\"\"+---+------------+\n",
    "| id|        name|\n",
    "+---+------------+\n",
    "|  2|Michael Rose|\n",
    "|  4|  Rames Rose|\n",
    "|  5|  Rames rose|\n",
    "+---+------------+\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091c4a6a",
   "metadata": {},
   "source": [
    "?? 8. Filter on an Array column\n",
    "* When you want to filter rows from DataFrame based on value present in an array collection column, you can use the first syntax. The below example uses array_contains() from Pyspark SQL functions which checks if a value contains in an array if present it returns true otherwise false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7889c786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+-----+------+\n",
      "|name            |languages         |state|gender|\n",
      "+----------------+------------------+-----+------+\n",
      "|{James, , Smith}|[Java, Scala, C++]|OH   |M     |\n",
      "|{Anna, Rose, }  |[Spark, Java, C++]|NY   |F     |\n",
      "+----------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import array_contains\n",
    "df.filter(array_contains(df.languages,\"Java\")) \\\n",
    "    .show(truncate=False)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1a09cc",
   "metadata": {},
   "source": [
    "9. Filtering on Nested Struct columns\n",
    "If your DataFrame consists of nested struct columns, you can use any of the above syntaxes to filter the rows based on the nested column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27f01f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------+-----+------+\n",
      "|name                  |languages   |state|gender|\n",
      "+----------------------+------------+-----+------+\n",
      "|{Julia, , Williams}   |[CSharp, VB]|OH   |F     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]|OH   |M     |\n",
      "+----------------------+------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Struct condition\n",
    "df.filter(df.name.lastname == \"Williams\") \\\n",
    "    .show(truncate=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4f8484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aae5bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61c08ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362615e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59da4a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd06a968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906b8a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d3c23b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026435ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae259dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df27f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
