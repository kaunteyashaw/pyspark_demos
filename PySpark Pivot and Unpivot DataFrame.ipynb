{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a980387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://lenovokspc:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-by-examples</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-by-examples>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession.builder.appName(\"pyspark-by-examples\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c70fd758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71a28a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Amount: long (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+-------+------+-------+\n",
      "|Product|Amount|Country|\n",
      "+-------+------+-------+\n",
      "|Banana |1000  |USA    |\n",
      "|Carrots|1500  |USA    |\n",
      "|Beans  |1600  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Banana |400   |China  |\n",
      "|Carrots|1200  |China  |\n",
      "|Beans  |1500  |China  |\n",
      "|Orange |4000  |China  |\n",
      "|Banana |2000  |Canada |\n",
      "|Carrots|2000  |Canada |\n",
      "|Beans  |2000  |Mexico |\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "#Create spark session\n",
    "data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n",
    "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n",
    "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n",
    "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n",
    "\n",
    "columns= [\"Product\",\"Amount\",\"Country\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6ccb59",
   "metadata": {},
   "source": [
    "    Pivot PySpark DataFrame\n",
    "    PySpark SQL provides pivot() function to rotate the data from one column into multiple columns. It is an aggregation where one of the grouping columns values is transposed into individual columns with distinct data. To get the total amount exported to each country of each product, will do group by Product, pivot by Country, and the sum of Amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12037a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Canada: long (nullable = true)\n",
      " |-- China: long (nullable = true)\n",
      " |-- Mexico: long (nullable = true)\n",
      " |-- USA: long (nullable = true)\n",
      "\n",
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico|USA |\n",
      "+-------+------+-----+------+----+\n",
      "|Orange |null  |4000 |null  |4000|\n",
      "|Beans  |null  |1500 |2000  |1600|\n",
      "|Banana |2000  |400  |null  |1000|\n",
      "|Carrots|2000  |1200 |null  |1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pivotDF = df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\n",
    "pivotDF.printSchema()\n",
    "pivotDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43bca9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.02711330000011\n",
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Banana: long (nullable = true)\n",
      " |-- Beans: long (nullable = true)\n",
      " |-- Carrots: long (nullable = true)\n",
      " |-- Orange: long (nullable = true)\n",
      "\n",
      "+-------+------+-----+-------+------+\n",
      "|Country|Banana|Beans|Carrots|Orange|\n",
      "+-------+------+-----+-------+------+\n",
      "|China  |400   |1500 |1200   |4000  |\n",
      "|USA    |1000  |1600 |1500   |4000  |\n",
      "|Mexico |null  |2000 |null   |null  |\n",
      "|Canada |2000  |null |2000   |null  |\n",
      "+-------+------+-----+-------+------+\n",
      "\n",
      "This will transpose the countries from DataFrame rows into columns and produces the below output. where ever data is not present, it represents as null by default.\n"
     ]
    }
   ],
   "source": [
    "startTimeQuery = time.perf_counter()\n",
    "pivotDF1 = df.groupBy(\"Country\").pivot(\"Product\").sum(\"Amount\")\n",
    "endTimeQuery = time.perf_counter()\n",
    "runTimeQuery = endTimeQuery - startTimeQuery\n",
    "print(runTimeQuery)\n",
    "pivotDF1.printSchema()\n",
    "pivotDF1.show(truncate=False)\n",
    "print(\"This will transpose the countries from DataFrame rows into columns and produces the below output. where ever data is not present, it represents as null by default.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07b89f5",
   "metadata": {},
   "source": [
    "    Pivot Performance improvement in PySpark 2.0 version 2.0 on-wards performance has been improved on Pivot, however, if you are using the lower version; note that pivot is a very expensive operation hence, it is recommended to provide column data (if known) as an argument to function as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d48f5064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+------+------+\n",
      "|Product|USA |China|Canada|Mexico|\n",
      "+-------+----+-----+------+------+\n",
      "|Orange |4000|4000 |null  |null  |\n",
      "|Beans  |1600|1500 |null  |2000  |\n",
      "|Banana |1000|400  |2000  |null  |\n",
      "|Carrots|1500|1200 |2000  |null  |\n",
      "+-------+----+-----+------+------+\n",
      "\n",
      "7.3973318999999265\n"
     ]
    }
   ],
   "source": [
    "startTimeQuery = time.perf_counter()\n",
    "countries = [\"USA\",\"China\",\"Canada\",\"Mexico\"]\n",
    "pivotDF = df.groupBy(\"Product\").pivot(\"Country\", countries).sum(\"Amount\")\n",
    "pivotDF.show(truncate=False)\n",
    "\n",
    "endTimeQuery = time.perf_counter()\n",
    "runTimeQuery = endTimeQuery - startTimeQuery\n",
    "print(runTimeQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27951ac6",
   "metadata": {},
   "source": [
    "Another approach is to do two-phase aggregation. PySpark 2.0 uses this implementation in order to improve the performance Spark-13749 <https://issues.apache.org/jira/browse/SPARK-13749>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03942bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.919432800000095\n",
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico|USA |\n",
      "+-------+------+-----+------+----+\n",
      "|Orange |null  |4000 |null  |4000|\n",
      "|Beans  |null  |1500 |2000  |1600|\n",
      "|Banana |2000  |400  |null  |1000|\n",
      "|Carrots|2000  |1200 |null  |1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "startTimeQuery = time.perf_counter()\n",
    "pivotDF = df.groupBy(\"Product\",\"Country\") \\\n",
    "      .sum(\"Amount\") \\\n",
    "      .groupBy(\"Product\") \\\n",
    "      .pivot(\"Country\") \\\n",
    "      .sum(\"sum(Amount)\") \n",
    "endTimeQuery = time.perf_counter()\n",
    "runTimeQuery = endTimeQuery - startTimeQuery\n",
    "print(runTimeQuery)\n",
    "pivotDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b931b5e1",
   "metadata": {},
   "source": [
    "    Unpivot PySpark DataFrame\n",
    "Unpivot is a reverse operation, we can achieve by rotating column values into rows values. PySpark SQL doesnâ€™t have unpivot function hence will use the stack() function. Below code converts column countries to row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c42dcf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Product|Country|Total|\n",
      "+-------+-------+-----+\n",
      "|Orange |China  |4000 |\n",
      "|Beans  |China  |1500 |\n",
      "|Beans  |Mexico |2000 |\n",
      "|Banana |Canada |2000 |\n",
      "|Banana |China  |400  |\n",
      "|Carrots|Canada |2000 |\n",
      "|Carrots|China  |1200 |\n",
      "+-------+-------+-----+\n",
      "\n",
      "+-------+-------+-----+\n",
      "|Product|Country|Total|\n",
      "+-------+-------+-----+\n",
      "| Orange|  China| 4000|\n",
      "|  Beans|  China| 1500|\n",
      "|  Beans| Mexico| 2000|\n",
      "| Banana| Canada| 2000|\n",
      "| Banana|  China|  400|\n",
      "|Carrots| Canada| 2000|\n",
      "|Carrots|  China| 1200|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import expr\n",
    "unpivotExpr = \"stack(3, 'Canada', Canada, 'China', China, 'Mexico', Mexico) as (Country,Total)\"\n",
    "unPivotDF = pivotDF.select(\"Product\", expr(unpivotExpr)) \\\n",
    "    .where(\"Total is not null\")\n",
    "unPivotDF.show(truncate=False)\n",
    "unPivotDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3ccf4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e926772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e07d8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabe9d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dbe9df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2df665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
