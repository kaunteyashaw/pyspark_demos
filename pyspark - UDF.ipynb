{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a926a14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://lenovokspc:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>app2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=app2>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession.builder.appName(\"app2\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark\n",
    "sc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6301048",
   "metadata": {},
   "source": [
    "Create a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d81b9d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec87a57",
   "metadata": {},
   "source": [
    " Create a Python Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec86effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convertCase(str):\n",
    "    resStr=\"\"\n",
    "    arr = str.split(\" \")\n",
    "    for x in arr:\n",
    "       resStr= resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n",
    "    return resStr \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543216d4",
   "metadata": {},
   "source": [
    "Convert a Python function to PySpark UDF\n",
    "\n",
    "\n",
    "\n",
    "Now convert this function convertCase() to UDF by passing the function to PySpark SQL udf(), this function is available at org.apache.spark.sql.functions.udf package. Make sure you import this package before using it.\n",
    "\n",
    "PySpark SQL udf() function returns org.apache.spark.sql.expressions.UserDefinedFunction class object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a4aba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "\"\"\" Converting function to UDF \"\"\"\n",
    "convertUDF = udf(lambda z: convertCase(z),StringType())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8784bf3e",
   "metadata": {},
   "source": [
    "Note: The default type of the udf() is StringType hence, you can also write the above statement without return type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34a6cf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" Converting function to UDF \n",
    "StringType() is by default hence not required \"\"\"\n",
    "convertUDF = udf(lambda z: convertCase(z)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a427de",
   "metadata": {},
   "source": [
    "    3. Using UDF with DataFrame\n",
    "    3.1 Using UDF with PySpark DataFrame select()\n",
    "    * Now you can use convertUDF() on a DataFrame column as a regular build-in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6600c2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|Seqno|Name         |\n",
      "+-----+-------------+\n",
      "|1    |John Jones   |\n",
      "|2    |Tracey Smith |\n",
      "|3    |Amy Sanders  |\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.select(col(\"Seqno\"), \\\n",
    "    convertUDF(col(\"Name\")).alias(\"Name\") ) \\\n",
    "   .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b913ed5b",
   "metadata": {},
   "source": [
    "3.2 Using UDF with PySpark DataFrame withColumn()\n",
    "* You could also use udf on DataFrame withColumn() function, to explain this I will create another upperCase() function which converts the input string to upper case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84804fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upperCase(str):\n",
    "    return str.upper()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b8f011",
   "metadata": {},
   "source": [
    "Let’s convert upperCase() python function to UDF and then use it with DataFrame withColumn(). Below example converts the values of “Name” column to upper case and creates a new column “Curated Name”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22f5ab27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-------------+\n",
      "|Seqno|Name        |Cureated Name|\n",
      "+-----+------------+-------------+\n",
      "|1    |john jones  |JOHN JONES   |\n",
      "|2    |tracey smith|TRACEY SMITH |\n",
      "|3    |amy sanders |AMY SANDERS  |\n",
      "+-----+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "upperCaseUDF = udf(lambda z:upperCase(z),StringType())   \n",
    "\n",
    "df.withColumn(\"Cureated Name\", upperCaseUDF(col(\"Name\"))) \\\n",
    "  .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4874c94",
   "metadata": {},
   "source": [
    "    3.3 Registering PySpark UDF & use it on SQL\n",
    "In order to use convertCase() function on PySpark SQL, you need to register the function with PySpark by using ```spark.udf.register()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57eb9ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|Seqno|Name         |\n",
      "+-----+-------------+\n",
      "|1    |John Jones   |\n",
      "|2    |Tracey Smith |\n",
      "|3    |Amy Sanders  |\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" Using UDF on SQL \"\"\"\n",
    "spark.udf.register(\"convertUDF\", convertCase,StringType())\n",
    "df.createOrReplaceTempView(\"NAME_TABLE\")\n",
    "spark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE\") \\\n",
    "     .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4f8db0",
   "metadata": {},
   "source": [
    "    4. Creating UDF using annotation\n",
    "In the previous sections, you have learned creating a UDF is a 2 step process,<br>\n",
    "***first, you need to create a Python function***,<br> \n",
    "__second convert function to UDF using SQL udf() function__,<br><br><br> however, you can avoid these two steps and create it with just a single step by using annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "979e6873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-------------+\n",
      "|Seqno|Name        |Cureated Name|\n",
      "+-----+------------+-------------+\n",
      "|1    |john jones  |JOHN JONES   |\n",
      "|2    |tracey smith|TRACEY SMITH |\n",
      "|3    |amy sanders |AMY SANDERS  |\n",
      "+-----+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@udf(returnType=StringType()) \n",
    "def upperCase(str):\n",
    "    return str.upper()\n",
    "\n",
    "df.withColumn(\"Cureated Name\", upperCase(col(\"Name\"))) \\\n",
    ".show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8f3e29",
   "metadata": {},
   "source": [
    "    5. Special Handling\n",
    "    5.1 Execution order\n",
    "    One thing to aware is in PySpark/Spark does not guarantee the order of evaluation of subexpressions meaning expressions are not guarantee to evaluated left-to-right or in any other fixed order. PySpark reorders the execution for query optimization and planning hence, AND, OR, WHERE and HAVING expression will have side effects.\n",
    "\n",
    "    So when you are designing and using UDF, you have to be very careful especially with null handling as these results runtime exceptions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f1af25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|Seqno|Name       |\n",
      "+-----+-----------+\n",
      "|1    |John Jones |\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" \n",
    "No guarantee Name is not null will execute first\n",
    "If convertUDF(Name) like '%John%' execute first then \n",
    "you will get runtime error\n",
    "\"\"\"\n",
    "spark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE \"+ \\\n",
    "          \"where Name is not null and convertUDF(Name) like '%John%'\") \\\n",
    "     .show(truncate=False)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c70852",
   "metadata": {},
   "source": [
    "    5.2 Handling null check\n",
    "UDF’s are error-prone when not designed carefully. for example, when you have a column that contains the value null on some records\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d87d2873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "|4    |null        |\n",
      "+-----+------------+\n",
      "\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\KAUNTEYA\\AppData\\Local\\Temp/ipykernel_17600/4075634363.py\", line 3, in convertCase\nAttributeError: 'NoneType' object has no attribute 'split'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17600/3323444052.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"NAME_TABLE2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"select convertUDF(Name) from NAME_TABLE2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m      \u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    500\u001b[0m                     \"Parameter 'truncate={}' should be either bool or int.\".format(truncate))\n\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint_truncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\KAUNTEYA\\AppData\\Local\\Temp/ipykernel_17600/4075634363.py\", line 3, in convertCase\nAttributeError: 'NoneType' object has no attribute 'split'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" null check \"\"\"\n",
    "\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\"),\n",
    "    ('4',None)]\n",
    "\n",
    "df2 = spark.createDataFrame(data=data,schema=columns)\n",
    "df2.show(truncate=False)\n",
    "df2.createOrReplaceTempView(\"NAME_TABLE2\")\n",
    "\n",
    "spark.sql(\"select convertUDF(Name) from NAME_TABLE2\") \\\n",
    "     .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e184fe42",
   "metadata": {},
   "source": [
    "Note that from the above snippet, record with “Seqno 4” has value “None” for “name” column. Since we are not handling null with UDF function, using this on DataFrame returns above error. Note that in Python None is considered null."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c671ff3a",
   "metadata": {},
   "source": [
    "    Below points to remember\n",
    "\n",
    "* Its always best practice to check for null inside a UDF function rather than checking for null outside.\n",
    "* In any case, if you can’t do a null check in UDF at lease use IF or CASE WHEN to check for null and call UDF conditionally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "093f55e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|_nullsafeUDF(Name)|\n",
      "+------------------+\n",
      "|John Jones        |\n",
      "|Tracey Smith      |\n",
      "|Amy Sanders       |\n",
      "|                  |\n",
      "+------------------+\n",
      "\n",
      "+-----+-----------+\n",
      "|Seqno|Name       |\n",
      "+-----+-----------+\n",
      "|1    |John Jones |\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"_nullsafeUDF\", lambda str: convertCase(str) if not str is None else \"\" , StringType())\n",
    "\n",
    "spark.sql(\"select _nullsafeUDF(Name) from NAME_TABLE2\") \\\n",
    "     .show(truncate=False)\n",
    "\n",
    "spark.sql(\"select Seqno, _nullsafeUDF(Name) as Name from NAME_TABLE2 \" + \\\n",
    "          \" where Name is not null and _nullsafeUDF(Name) like '%John%'\") \\\n",
    "     .show(truncate=False)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d489bf",
   "metadata": {},
   "source": [
    "    5.3 Performance concern using UDF\n",
    "UDF’s are a black box to PySpark hence it can’t apply optimization and you will lose all the optimization PySpark does on Dataframe/Dataset. When possible you should use Spark SQL built-in functions as these functions provide optimization. Consider creating UDF only when existing built-in SQL function doesn’t have it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a0710c",
   "metadata": {},
   "source": [
    "    #Conclusion\n",
    "In this article, you have learned the following\n",
    "\n",
    "PySpark UDF is a User Defined Function that is used to create a reusable function in Spark.\n",
    "Once UDF created, that can be re-used on multiple DataFrames and SQL (after registering).\n",
    "The default type of the udf() is StringType.\n",
    "You need to handle nulls explicitly otherwise you will see side-effects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
